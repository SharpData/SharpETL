# ETLLogger 日志单独输出
log4j.logger.ETLLogger=INFO, console, ETLLogger, infoRollingFile
log4j.additivity.ETLLogger=false
log4j.appender.ETLLogger=org.apache.log4j.RollingFileAppender
log4j.appender.ETLLogger.File=./logs/ETLLogger.log
log4j.appender.ETLLogger.Append=true
log4j.appender.ETLLogger.MaxFileSize=16MB
log4j.appender.ETLLogger.MaxBackupIndex=3
log4j.appender.ETLLogger.layout=org.apache.log4j.PatternLayout
log4j.appender.ETLLogger.layout.ConversionPattern=%d{yyyy/MM/dd HH:mm:ss} %-5p - %m%n
log4j.appender.ETLLogger.Encoding=UTF-8

# 全局日志
log4j.rootLogger=ERROR, infoRollingFile, console

# 控制台输出
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.out
log4j.appender.console.Threshold=INFO
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yyyy/MM/dd HH:mm:ss} %-5p [%c] - %m%n
log4j.appender.console.Encoding=UTF-8

# info 级别滚动文件日志输出
log4j.appender.infoRollingFile=org.apache.log4j.RollingFileAppender
log4j.appender.infoRollingFile.File=./logs/info.log
log4j.appender.infoRollingFile.Threshold=INFO
log4j.appender.infoRollingFile.Append=true
log4j.appender.infoRollingFile.MaxFileSize=16MB
log4j.appender.infoRollingFile.MaxBackupIndex=3
log4j.appender.infoRollingFile.layout=org.apache.log4j.PatternLayout
log4j.appender.infoRollingFile.layout.ConversionPattern=%d{yyyy/MM/dd HH:mm:ss} %-5p [%c] - %m%n
log4j.appender.infoRollingFile.Encoding=UTF-8

shell.log.level=WARN
log4j.logger.org.spark-project.jetty=WARN
log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
log4j.logger.org.apache.spark.repl.Main=${shell.log.level}
log4j.logger.org.apache.spark.api.python.PythonGatewayServer=${shell.log.level}
log4j.logger.org.apache.spark.ContextCleaner=ERROR
log4j.logger.org.apache.hadoop.mapreduce=ERROR
log4j.logger.org.apache.hadoop.hive=ERROR
