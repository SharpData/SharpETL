etl.workflow.path=tasks
etl.default.jobId.column=job_id
etl.default.jobTime.column=job_time
flyway.driver=com.mysql.cj.jdbc.Driver
flyway.url=jdbc:mysql://localhost/sharp_etl?useUnicode=true&characterEncoding=UTF-8&serverTimezone=Asia/Shanghai&autoReconnect=true
flyway.username=root
flyway.password=root
spark.default.spark.sql.adaptive.enabled=true
spark.default.spark.sql.adaptive.logLevel=info
spark.default.spark.sql.adaptive.advisoryPartitionSizeInBytes=128m
spark.default.spark.sql.adaptive.coalescePartitions.enabled=true
spark.default.spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.default.spark.sql.adaptive.fetchShuffleBlocksInBatch=true
spark.default.spark.sql.adaptive.localShuffleReader.enabled=true
spark.default.spark.sql.adaptive.skewJoin.enabled=true
spark.default.spark.sql.adaptive.skewJoin.skewedPartitionFactor=5
spark.default.spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=400m
spark.default.spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0.2
spark.default.spark.sql.autoBroadcastJoinThreshold=-1
spark.default.spark.sql.adaptive.shuffle.targetPostShuffleInputSize=134217728
spark.default.hive.exec.dynamic.partition=true
spark.default.hive.exec.dynamic.partition.mode=nonstrict
spark.default.spark.sql.sources.partitionOverwriteMode=dynamic
spark.default.spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.default.spark.kryoserializer.buffer.max=128m
spark.default.spark.sql.crossJoin.enabled=true
spark.default.spark.driver.cores=1
spark.default.spark.driver.memory=1g
spark.default.spark.driver.memoryOverhead=1g
spark.default.spark.driver.maxResultSize=0
spark.default.spark.executor.cores=2
spark.default.spark.executor.memory=4g
spark.default.spark.executor.memoryOverhead=2g
spark.default.spark.dynamicAllocation.enabled=true
spark.default.spark.shuffle.service.enabled=true
spark.default.spark.dynamicAllocation.minExecutors=1
spark.default.spark.dynamicAllocation.maxExecutors=4
spark.default.spark.streaming.stopGracefullOnShutdown=true
spark.default.spark.streaming.backpressure.enable=true
spark.default.spark.streaming.kafka.maxRatePerPartition=100000

local_test.mysql.url=jdbc:mysql://localhost:2333/local_test
local_test.mysql.user=root
local_test.mysql.password=root
local_test.mysql.driver=com.mysql.cj.jdbc.Driver
local_test.mysql.fetchsize=1000

sales.postgres.url=jdbc:postgresql://localhost:5432/postgres?stringtype=unspecified
sales.postgres.user=postgres
sales.postgres.password=postgres
sales.postgres.driver=org.postgresql.Driver
sales.postgres.fetchsize=10

postgres.postgres.url=jdbc:postgresql://localhost:5432/postgres?stringtype=unspecified
postgres.postgres.user=postgres
postgres.postgres.password=postgres
postgres.postgres.driver=org.postgresql.Driver
postgres.postgres.fetchsize=10

sysmaster.informix.url=jdbc:informix-sqli://localhost:9088/sysmaster:INFORMIXSERVER=informix;DELIMIDENT=Y
sysmaster.informix.user=informix
sysmaster.informix.password=in4mix
sysmaster.informix.driver=com.informix.jdbc.IfxDriver
sysmaster.informix.fetchsize=100

kafka.producer.kafka.bootstrap.servers=localhost:9092

kafka.consumer.kafka.bootstrap.servers=localhost:9092
kafka.consumer.startingOffsets=earliest
